{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: \n",
    "* Implement the function to compute the loss and gradient for multi-class linear SVM with the bias trick. (the \"svm_loss_forloop\" function in linear_svm.py)\n",
    "* Understand how to speed the computation with vectorized implementation (the \"svm_loss_vectorized\" function in linear_svm.py)\n",
    "* Implement Stochastic Gradient Descent in \"LinearSVM.train\" and the predict function \"LinearSVM.predict\" in linear_svm.py\n",
    "* Find the optimal learning rate and regularization weight using validation set (In this notebook)\n",
    "\n",
    "## Notice:\n",
    "There are a couple of new files we need for this week.\n",
    "* You need to download and unzip this [classifiers](https://drive.google.com/open?id=0B2Yvyjb5-_OgQ29PTzcwRWN0cEE) folder into the \"notebooks\" folder. So it looks as \"notebooks/classifiers/\"\n",
    "* You need to download this [\"gradient_check.py\"](https://drive.google.com/open?id=0B2Yvyjb5-_OgWjVhdVh2NWN1bVU) file and add it to \"notebooks/utils\". This is a great piece of code from the Stanford CS231n class for comparing analytical gradient with numerical gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_utils import load_CIFAR10\n",
    "from classifiers.linear_svm import svm_loss_bias_forloop\n",
    "from classifiers.linear_svm import svm_loss_forloop\n",
    "from classifiers.linear_svm import svm_loss_vectorized\n",
    "from utils.gradient_check import grad_check_sparse\n",
    "from classifiers.linear_svm import LinearSVM\n",
    "\n",
    "import time\n",
    "\n",
    "# Some magic so that the notebook will automatically reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR10 ...\n",
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '../opt/data/datasets/cifar-10-batches-py'\n",
    "print('Loading CIFAR10 ...')\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print 'Training data shape: ', X_train.shape\n",
    "print 'Training labels shape: ', y_train.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (2000, 32, 32, 3)\n",
      "Train labels shape:  (2000,)\n",
      "Validation data shape:  (100, 32, 32, 3)\n",
      "Validation labels shape:  (100,)\n",
      "Test data shape:  (100, 32, 32, 3)\n",
      "Test labels shape:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise.\n",
    "num_training = 2000\n",
    "num_validation = 100\n",
    "num_test = 100\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 8.981959 \n"
     ]
    }
   ],
   "source": [
    "# Compute multi-class SVM loss and gradient with the given svm_loss_bias_forloop\n",
    "# We just use some random weights here\n",
    "W = np.random.randn(3072, 10) * 0.0001\n",
    "b = np.random.randn(10) * 0.0001\n",
    "loss, grad_W, grad_b = svm_loss_bias_forloop(W, b, X_train, y_train, 0.00001)\n",
    "print 'loss : %f ' % loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -20.173028 analytic: -20.173028, relative error: 2.158456e-11\n",
      "numerical: 1.394145 analytic: 1.394145, relative error: 1.536613e-10\n",
      "numerical: 23.427459 analytic: 23.427459, relative error: 4.812531e-12\n",
      "numerical: 4.430796 analytic: 4.430796, relative error: 1.617734e-10\n",
      "numerical: 18.190827 analytic: 18.211582, relative error: 5.701679e-04\n",
      "grad_check_sparse W in 9.281243s\n"
     ]
    }
   ],
   "source": [
    "# Here we check the analaytic gradient by comparing it to the numerical gradient\n",
    "# \"grad_check_sparse\" numerically computes the gradient along several randomly chosen dimensions, and\n",
    "# compute with difference with the analytical gradient from svm_loss_bias_forloop. \n",
    "# understand how this is implemented in utils/gradient_check.py\n",
    "\n",
    "loss, grad_W, grad_b = svm_loss_bias_forloop(W, b, X_train, y_train, 0.00001)\n",
    "f = lambda W: svm_loss_bias_forloop(W, b, X_train, y_train, 0.00001)[0]\n",
    "tic = time.time()\n",
    "grad_numerical = grad_check_sparse(f, W, grad_W, 5)\n",
    "toc = time.time()\n",
    "print 'grad_check_sparse W in %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement your own computation for loss & gradient with the Bias Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3073)\n",
      "(100, 3073)\n",
      "(100, 3073)\n"
     ]
    }
   ],
   "source": [
    "# To use the bias trick, we simply append the bias dimension of ones so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3073,10) (3072,10) (3073,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-198ecb5f7972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# You are allowed to use forloop (as in svm_loss_bias_forloop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_loss_forloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mt_forloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/classifiers/linear_svm.py\u001b[0m in \u001b[0;36msvm_loss_forloop\u001b[0;34m(W, X, y, reg, delta)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Do the same for d_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0md_W\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0md_W\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3073,10) (3072,10) (3073,10) "
     ]
    }
   ],
   "source": [
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "# Implement svm_loss_forloop in classifiers/linear_svm.py\n",
    "# You are allowed to use forloop (as in svm_loss_bias_forloop)\n",
    "tic = time.time()\n",
    "loss, grad = svm_loss_forloop(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "t_forloop = toc - tic\n",
    "\n",
    "print loss\n",
    "print t_forloop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check your loss\n",
    "# svm_loss_vectorized is a given fast (vectorized) implementation\n",
    "# you svm_loss_forloop function should return the same result (but slower) \n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = svm_loss_vectorized(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "t_vectorized = toc - tic\n",
    "\n",
    "# Correct implemention will give zero difference\n",
    "print \"loss : %f, loss_vectorized: %f, difference: %f\" % (loss, loss_vectorized, loss - loss_vectorized)\n",
    "# Notice vectorized implementation will give significant speed up\n",
    "print \"time forloop: %f, time_vectorized: %f\" % (t_forloop, t_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check your gradient\n",
    "# you should see small relative error\n",
    "loss, grad = svm_loss_forloop(W, X_train, y_train, 0.00001)\n",
    "f = lambda w: svm_loss_forloop(w, X_train, y_train, 0.00001)[0]\n",
    "tic = time.time()\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)\n",
    "toc = time.time()\n",
    "print 'grad_check_sparse in %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 2: Vectorized Implementation\n",
    "\n",
    "**Check the vectorized implementation (svm_loss_vectorized) in classifiers/linear_svm**\n",
    "\n",
    "**In particular, make sure you can explain how broadcasting and advance indexing are used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Implement Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now implement the LinearSVM.train() in classifiers/linear_svm.py\n",
    "# It does stochastic gradient descent\n",
    "# Run it with the code below\n",
    "# You should see the loss decrease with numbers of interations\n",
    "svm = LinearSVM()\n",
    "tic = time.time() \n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=2000, verbose=True)  \n",
    "toc = time.time()\n",
    "print 'Time spent %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also lot the loss as a function of iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement the LinearSVM.predict() function in classifiers/linear_svm.py \n",
    "# use this block of code to evaluate your classifier on both the training and validation set. \n",
    "# You should get around 0.40 accuracy on the training set and 0.34 accuracy on the validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also visualize the weights in the learned SVM\n",
    "w = svm.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Hyper-Parameters (learninng rate and regularization strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let see how to choose the best \"learning_rate\" and \"reg\" for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of about 0.4 on the validation set.\n",
    "learning_rates = [1e-7, 2e-7, 3e-7, 5e-5, 8e-7]\n",
    "regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]\n",
    "num_iters=500 \n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
    "best_lr = 0\n",
    "best_reg = 0\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        svm = LinearSVM()\n",
    "        # your code \n",
    "        \n",
    "        # Train the svm with the given hyperparameters\n",
    "        svm.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                      num_iters=num_iters, verbose=False)\n",
    "        \n",
    "        # Predict classes of the training and test instances\n",
    "        y_train_pred = svm.predict(X_train)\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        \n",
    "        # Calculate the accuracy of the train- and test-predictions\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        results[(lr, rs)] = (acc_train, acc_val)\n",
    "        \n",
    "        # Update best values if the current validation accuracy\n",
    "        # has been increased\n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_lr = lr\n",
    "            best_reg = rs\n",
    "            best_svm = svm\n",
    "        \n",
    "        print((lr, rs, acc_train, acc_val))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "print 'best validation accuracy achieved during cross-validation: %e' % best_val\n",
    "print 'with learning rate %e, regularization strength %e' % (best_lr, best_reg)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now train with more interations with the optimal parameters\n",
    "svm = LinearSVM()\n",
    "tic = time.time() \n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=best_lr, reg=best_reg,\n",
    "                      num_iters=2000, verbose=True)  \n",
    "toc = time.time()\n",
    "print 'Time spent %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize W\n",
    "w = svm.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])\n",
    "plt.show()\n",
    "\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )\n",
    "y_test_pred = svm.predict(X_test)\n",
    "print 'testing accuracy: %f' % (np.mean(y_test == y_test_pred), )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Test with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Congradulations! You just implement the entire classification pipeline on your own!!!\n",
    "# It is truly amazing that we are able to achieve around 36% accuracy \n",
    "# This is over 25% increasement from random guess (10 classes)\n",
    "# Now try to re-trian your SVM with more data by setting larger values to the following three variables\n",
    "\n",
    "cifar10_dir = '../opt/data/datasets/cifar-10-batches-py'\n",
    "print('Loading CIFAR10 ...')\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "#np.random.seed(42)\n",
    "#np.random.shuffle(X_train)\n",
    "#np.random.seed(42)\n",
    "#np.random.shuffle(y_train)\n",
    "\n",
    "num_training = 20000\n",
    "num_validation = 7000\n",
    "num_test = 7000\n",
    "\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# To use the bias trick, we simply append the bias dimension of ones so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "\n",
    "X_train_and_val = np.vstack((X_train,X_val))\n",
    "y_train_and_val = np.concatenate((y_train,y_val))\n",
    "\n",
    "\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_lr = 0\n",
    "best_reg = 0\n",
    "\n",
    "\n",
    "learning_rates = [1e-7, 2e-7, 3e-7, 5e-5, 8e-7]\n",
    "regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]\n",
    "num_iters=2000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        svm = LinearSVM()\n",
    "        svm.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                      num_iters=num_iters, verbose=False)\n",
    "        y_train_pred = svm.predict(X_train)\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        \n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_lr = lr\n",
    "            best_reg = rs\n",
    "        \n",
    "        print((lr, rs, acc_train, acc_val))\n",
    "\n",
    "\n",
    "svm = LinearSVM()\n",
    "tic = time.time() \n",
    "loss_hist = svm.train(X_train_and_val, y_train_and_val, learning_rate=best_lr, reg=best_reg,\n",
    "                      num_iters=2000, verbose=False)\n",
    "toc = time.time()\n",
    "print 'Time spent training: %fs' % (toc - tic)\n",
    "\n",
    "# Visualize W\n",
    "w = svm.W[:-1,:] # strip out the bias\n",
    "print w.shape\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])\n",
    "plt.show()\n",
    "\n",
    "y_train_pred = svm.predict(X_train_and_val)\n",
    "print 'training accuracy: %f' % (np.mean(y_train_and_val == y_train_pred), )\n",
    "y_test_pred = svm.predict(X_test)\n",
    "print 'testing accuracy: %f' % (np.mean(y_test == y_test_pred), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
